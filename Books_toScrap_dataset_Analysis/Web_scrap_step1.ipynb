{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ***Web Scraping using BeautifulSoup-Python (STEP 1)***"
      ],
      "metadata": {
        "id": "hg9YZL26EUpd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Web Scraping to collect Data from Books.toscrape.com Website which will then be used to for making ML Model and doing EDA , for story telling using Power BI."
      ],
      "metadata": {
        "id": "FRRy24gyD4VE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLp9CnELC3l_",
        "outputId": "a1d40eaf-7555-47fe-f736-97ecf8a24755"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Page 1 scraped\n",
            "✅ Page 2 scraped\n",
            "✅ Page 3 scraped\n",
            "✅ Page 4 scraped\n",
            "✅ Page 5 scraped\n",
            "✅ Page 6 scraped\n",
            "✅ Page 7 scraped\n",
            "✅ Page 8 scraped\n",
            "✅ Page 9 scraped\n",
            "✅ Page 10 scraped\n",
            "✅ Done. All data saved to cleaned_books.csv\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "base_url = \"https://books.toscrape.com/catalogue/page-{}.html\"\n",
        "books = []\n",
        "\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n",
        "}\n",
        "\n",
        "for page in range(1, 11):  # scrape all 10 pages\n",
        "    url = base_url.format(page)\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers)\n",
        "        response.raise_for_status()\n",
        "        response.encoding = 'utf-8'\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Failed to fetch page {page}: {e}\")\n",
        "        continue\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    articles = soup.find_all('article', class_='product_pod')\n",
        "\n",
        "    for book in articles:\n",
        "        title = book.h3.a['title']\n",
        "\n",
        "        price_raw = book.find('p', class_='price_color').text.strip()\n",
        "        price_clean = ''.join(ch for ch in price_raw if ch.isdigit() or ch == '.')\n",
        "        price = float(price_clean)\n",
        "\n",
        "        availability = book.find('p', class_='instock availability').text.strip()\n",
        "        rating = book.p['class'][1]\n",
        "        link = \"https://books.toscrape.com/catalogue/\" + book.h3.a['href']\n",
        "\n",
        "        books.append({\n",
        "            'title': title,\n",
        "            'price (£)': price,\n",
        "            'availability': availability,\n",
        "            'rating': rating,\n",
        "            'link': link\n",
        "        })\n",
        "\n",
        "    print(f\"✅ Page {page} scraped\")\n",
        "    time.sleep(1.5)  # reduce load on server\n",
        "\n",
        "df = pd.DataFrame(books)\n",
        "df.to_csv(\"cleaned_books.csv\", index=False)\n",
        "print(\"✅ Done. All data saved to cleaned_books.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TA5NFe0QEqXp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}